{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqbSDQPOnCqr"
   },
   "source": [
    "# Objective: To build a hand gesture recognition model using neural netwroks.\n",
    "* Each gesture can be assigned to a unique action which can be integrated in smart devices having camera.\n",
    "* Camera will input our gesture to our embedded neural model which inturn will perform gesture classificartion. Then corresponding action to this predicted gesture can be performed. E.g. We can embed this neural model into smart T.V. and perform basic functions without remote, like turning volume up by showing thumbs up, changing channels by simply right/left swiping etc.\n",
    "* Basically in this task we will train neural netwroks for classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw_wUt7dnGpz"
   },
   "source": [
    "## INDEX\n",
    "\n",
    "* [Data Sourcing](#Data-Sourcing)\n",
    "* [Creating Generator](#Creating-Generator)\n",
    "* [Basic EDA](#Basic-EDA)\n",
    "* [Model Creation](#Model-Creation)\n",
    "    1. [Choosing frame shape](#Choosing-frame-shape)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    2. [Choosing sample numbers](#Choosing-sample-numbers)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    3. [Choosing optimizer](#Choosing-optimizer)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    4. [Training Conv3D Model with higher epocs](#Training-Conv3D-Model-with-higher-epocs)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    5. [Builing Conv3D Model with augmentation](#Builing-Conv3D-Model-with-augmentation)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    6. [Custom CNN + GRU Model](#Custom-CNN-+-GRU-Model)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    7. [Resnet50 + GRU Model](#Resnet50-+-GRU-Model)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    8. [Resnet50 + GRU Model with augmentation](#Resnet50-+-GRU-Model-with-augmentation)\n",
    "        * [Conclusion](#Conclusion)\n",
    "    9. [VGG16 + GRU with data augmentation](#VGG16-+-GRU-with-data-augmentation)\n",
    "        * [Conclusion](#Conclusion)\n",
    "\n",
    "* [Final Model Selection](#Final-Model-Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing liabraries for data processing and visualization:\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import random as rn\n",
    "import shutil\n",
    "import zipfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Importing Deep learning liabraries:\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam,SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFNds-JGnCqy"
   },
   "outputs": [],
   "source": [
    "# We set the random seed so that we can reproduce the same randomness and thus results won't vary drastically.\n",
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)\n",
    "\n",
    "# For supressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sourcing\n",
    "* `The training data consists of a few hundred videos categorised into one of the five classes`: \n",
    "    - Thumbs up:  Increase the volume\n",
    "    - Thumbs down: Decrease the volume\n",
    "    - Left swipe: 'Jump' backwards 10 seconds\n",
    "    - Right swipe: 'Jump' forward 10 seconds  \n",
    "    - Stop: Pause the movie\n",
    "* `Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images)`. These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use. \n",
    "* Dataset contains a 'train' and a 'val' folder with two CSV files for the two folders:\n",
    "    * These folders are in turn divided into subfolders where each subfolder represents a video of a particular gesture. `Each subfolder, i.e. a video, contains 30 frames (or images)`.  \n",
    "    * `Each row of the CSV file represents one video and contains three main pieces of information - the name of the subfolder containing the 30 images of the video, the name of the gesture and the numeric label (between 0-4) of the video`.\n",
    "* Note that all images in a particular video subfolder have the same dimensions but different videos may have different dimensions.\n",
    "* These videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data augmentation\n",
    "def aug(source_path):\n",
    "    \n",
    "    path1 = './Project_data/aug'\n",
    "    if not os.path.isdir( path1 ) :\n",
    "            os.mkdir( path1 )\n",
    "    \n",
    "    train_record_list = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "    for record in train_record_list:\n",
    "        \n",
    "        frame_list = os.listdir(source_path+'/'+ (record.split(';')[0]))\n",
    "        path2 = './Project_data/aug/'+ 'aug_'+ record.split(';')[0]\n",
    "        if not os.path.isdir( path2 ) :\n",
    "            os.mkdir( path2 )\n",
    "        for frame in frame_list:\n",
    "            image = cv2.imread(source_path+'/'+ record.split(';')[0]+'/'+frame).astype(np.float32)\n",
    "            GaussianBlur = cv2.GaussianBlur(image,(3,3),0)   # Adding gaussian blur\n",
    "            fliped= cv2.flip(GaussianBlur,1)\n",
    "            dir_path = path2 + \"/\"  + frame\n",
    "            status = cv2.imwrite(dir_path, fliped)\n",
    "        \n",
    "        with open('aug_csv.csv', \"a\", newline=\"\") as aug_file:\n",
    "            aug_writer = csv.writer(aug_file, delimiter=';')\n",
    "            if int(record.split(';')[2]) == 0:\n",
    "                aug_writer.writerow(['aug_'+ record.split(';')[0], 'Right_Swipe_new', '1'])\n",
    "            elif int(record.split(';')[2]) == 1:\n",
    "                aug_writer.writerow(['aug_'+ record.split(';')[0], 'Left Swipe_new', '0'])\n",
    "            else:\n",
    "                aug_writer.writerow(['aug_'+ record.split(';')[0], record.split(';')[1], record.split(';')[2].replace('\\n','')])\n",
    "                \n",
    "    source_dir = path1\n",
    "    target_dir = './Project_data/train'\n",
    "\n",
    "    file_names = os.listdir(source_dir)\n",
    "    for file_name in file_names:\n",
    "        shutil.move(os.path.join(source_dir, file_name), target_dir)\n",
    "\n",
    "    with open(\"./Project_data/train.csv\", \"a\") as file1,open(\"./aug_csv.csv\", \"r\") as file2:\n",
    "        for line in file2:\n",
    "           file1.write(line)\n",
    "\n",
    "    if os.path.exists(path1):\n",
    "        shutil.rmtree(path1)\n",
    "\n",
    "    if os.path.exists(\"./aug_csv.csv\"):\n",
    "        os.remove(\"./aug_csv.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for referencing train and validation data path:\n",
    "train_doc = None\n",
    "val_doc = None\n",
    "train_path = None\n",
    "val_path = None\n",
    "num_train_sequences = None\n",
    "num_val_sequences = None\n",
    "\n",
    "def read_path(augment):\n",
    "    global train_doc\n",
    "    global val_doc\n",
    "    global train_path\n",
    "    global val_path\n",
    "    global num_train_sequences\n",
    "    global num_val_sequences\n",
    "    \n",
    "    if os.path.exists('./Project_data'):\n",
    "        shutil.rmtree('./Project_data')\n",
    "    \n",
    "    with zipfile.ZipFile('./Project_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "    \n",
    "    train_path = './Project_data/train'      # path to train set\n",
    "    val_path = './Project_data/val'          # path to validation set\n",
    "    \n",
    "    if augment == True:\n",
    "        aug(train_path)\n",
    "        \n",
    "    # Reading rows of CSV file (videos indirectly) at random order to avoid any introduction of bias when training:\n",
    "    train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "    val_doc = np.random.permutation(open('./Project_data/val.csv').readlines()) \n",
    "    \n",
    "    num_train_sequences = len(train_doc)\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc)\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-cIGIk3nCq1"
   },
   "source": [
    "### Creating Generator\n",
    "\n",
    "* We will have to create our own custom data generator since we are working with videos and not regualar text or image data.\n",
    "* Data preprocessing:\n",
    "    * Since images are of varying shape, we must process them as a part of data formatting or else conv3D will throw error if the inputs in a batch have different shapes    \n",
    "        - We have not decided to crop as useful information may be present at any location of frame.\n",
    "        - `We will resize the image instead of cropping`.\n",
    "    * Normalizing th epixel values :\n",
    "        - Since the image is natural image we can divide each channel pixel value by 255 (i.e. {2^8}-1 ,here 8 because of unsigned int format)\n",
    "        - But we have decided to use min-max normalization.\n",
    "        - We acknowledge that type of normalization will depend upon nature of image and is subject to task at hand.\n",
    "* Data Augmentation:\n",
    "    * We will try to make model more general by increasing the size of training data with the help of Data Augmentation.\n",
    "        - we have decided to add gaussian noise to the image\n",
    "        - We will also flip the image around y-axis as we will be able to augment the additional oppposite guesture using same person for swiping. Also, we will able to augment guestures for other hand thus increasing variation in the data.\n",
    "            - We must correct the labels for augmented swipe guestures as left swipe will become right and vice versa.\n",
    "        - We could also perform image rotation( upto very little degree ), but we will not implement it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJVZ8BkynCq3"
   },
   "outputs": [],
   "source": [
    "# Generator function:\n",
    "def generator(source_path, folder_list, batch_size, input_shape, sampling_type):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size,'; sampling type =',sampling_type)\n",
    "    if sampling_type == 'middle':\n",
    "        img_idx = list(range(5,25))\n",
    "    elif sampling_type == 'custom':\n",
    "        img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),input_shape[1],input_shape[2],input_shape[3])) \n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                frame_list = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate over the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+frame_list[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #since the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    image = cv2.resize(image,input_shape[1:3],interpolation = cv2.INTER_AREA)   # Resizing the image\n",
    "                    image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX) #Normalising the image pixel values\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]       #feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]       #feed in the image \n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]       #feed in the image\n",
    "                    \n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                \n",
    "            yield batch_data, batch_labels #yield the batch_data and the batch_labels\n",
    "\n",
    "        \n",
    "        # Code for the remaining data points which are left after full batches\n",
    "        remaining_datapoints = len(folder_list) % batch_size\n",
    "        if remaining_datapoints != 0:\n",
    "            batch += 1\n",
    "            batch_data = np.zeros((remaining_datapoints,len(img_idx),input_shape[1],input_shape[2],input_shape[3])) \n",
    "            batch_labels = np.zeros((remaining_datapoints,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(remaining_datapoints): # iterate over the batch_size\n",
    "                    frame_list = os.listdir(source_path+'/'+ t[folder + ((batch)*remaining_datapoints)].split(';')[0]) # read all the images in the folder\n",
    "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                        image = cv2.imread(source_path+'/'+ t[folder + ((batch)*remaining_datapoints)].strip().split(';')[0]+'/'+frame_list[item]).astype(np.float32)\n",
    "\n",
    "                        #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                        #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                        image = cv2.resize(image,input_shape[1:3],interpolation = cv2.INTER_AREA)  # Resizing the image\n",
    "                        image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX) #Normalising the image pixel values\n",
    "\n",
    "                        batch_data[folder,idx,:,:,0] = image[:,:,0]           #feed in the image\n",
    "                        batch_data[folder,idx,:,:,1] = image[:,:,1]           #feed in the image\n",
    "                        batch_data[folder,idx,:,:,2] = image[:,:,2]           #feed in the image\n",
    "                        \n",
    "                        \n",
    "                    batch_labels[folder, int(t[folder + ((batch)*remaining_datapoints)].strip().split(';')[2])] = 1\n",
    "                    \n",
    "            yield batch_data, batch_labels #yield the batch_data and the batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will try with SGD/Adam optizer.\n",
    "* Since our target variable is categorical and our task is multiclass classification, we will use Categorical_crossentropy as a loss function and categorical_accuracy as a metric for model evalution.\n",
    "* We will make use of ModelCheckpoint functionality of tf.keras liabrary to save a model/weights at some interval, so the model/weights can be loaded later to continue the training from the state saved. \n",
    "    - To reduce disk space while saving model weights we will only save best weights using `save_best_only` parameter.\n",
    "* we will also be using 'ReduceLROnPlateau' feature of keras which reduces the learning rate when a specified metric has stopped improving even after specified number of epochs.\n",
    "    - patience=2 means we will wait for 2 epochs before reducing learning rate when metric has stopped improving.\n",
    "    - factor is a quantity by which the learning rate will be reduced.\n",
    "* We will monitor 'validation loss' for both callback parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for compliling the model and training:\n",
    "def run(ModelName,batch_size,num_epochs,optimzr,sampling_type):\n",
    "    curr_dt_time = datetime.datetime.now()\n",
    "    if optimzr == 'adam':\n",
    "        optimiser = tf.keras.optimizers.Adam()\n",
    "    elif optimzr == 'sgd':\n",
    "        optimiser = tf.keras.optimizers.SGD()\n",
    "    \n",
    "    ModelName.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    print (ModelName.summary())\n",
    "    \n",
    "    train_generator = generator(train_path, train_doc, batch_size,input_shape,sampling_type)\n",
    "    val_generator = generator(val_path, val_doc, batch_size,input_shape,sampling_type)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    model_name = ModelName._name + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "    if not os.path.exists(model_name):\n",
    "        os.mkdir(model_name)\n",
    "        \n",
    "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "    \n",
    "    callbacks_list = [checkpoint, LR]\n",
    "    \n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "        \n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "    history = ModelName.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "    \n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots train and validation accuracy and loss.\n",
    "def visualize_performance(model_history):\n",
    "    acc = model_history.history['categorical_accuracy']\n",
    "    val_acc = model_history.history['val_categorical_accuracy']\n",
    "\n",
    "    loss = model_history.history['loss']\n",
    "    val_loss = model_history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(model_history.epoch))\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XOfvDLLnCq5",
    "outputId": "882b349b-2018-4418-94d0-a2641c32707a"
   },
   "outputs": [],
   "source": [
    "read_path(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = cv2.imread('./Project_data/train/WIN_20180907_15_35_09_Pro_Right Swipe_new/WIN_20180907_15_35_09_Pro_00012.png')\n",
    "print('original shape is :',random_image.shape)\n",
    "plt.imshow(cv2.cvtColor(random_image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "resize_random_image = cv2.resize(random_image,(100,100),interpolation = cv2.INTER_AREA)\n",
    "print('Shape after resizing: ',resize_random_image.shape)\n",
    "plt.imshow(cv2.cvtColor(resize_random_image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "gusassian_random = cv2.GaussianBlur(resize_random_image,(3,3),0) \n",
    "print('After adding gaussian blur:  ')\n",
    "plt.imshow(cv2.cvtColor(gusassian_random, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "gusassian_random_flip = cv2.flip(gusassian_random,1)\n",
    "print('After flipping an image:  ')\n",
    "plt.imshow(cv2.cvtColor(gusassian_random_flip, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "* our data is made of videos and thus is sequential in nature. Thus we will use following two types of architectures :\n",
    "    * CNN + RNN architecture :\n",
    "        - Videos are nothing but sequence of images. Thus we can pass these sequence of images through a 2D-CNN which extracts a feature vector for each image (handles spatial information).\n",
    "        - we then pass these extracted feature i.e. output of CNN to the RNN to handle the temporal information.\n",
    "            - Since LSU has less parameters to train it is better than LSTM in terms of computational efficiency(lstm has 4 times more trainable parameter than vanilla RNN whereas LSU has 3 times more trainable parameter than vanilla RNN. note: This is not true for keras implementation of LSU as it has twice bias parametrs.)\n",
    "            - We will choose LSU for modelling.\n",
    "        - Since our ultimate task is to classify among 5 classes of gesture, we pass the output of rnn to softmax function.\n",
    "    * 3D convolutional network :\n",
    "        - 3D-CNN is just natural extension to the 2D convolutions. Only difference between them and 2D-CNN is that in 3D-CNN, you move the filter in three directions instead of just two. i.e. you move the filter across the sequence of image as well.\n",
    "        - Since our ultimate task is to classify among 5 classes of gesture, we use softmax function at the end.\n",
    "        \n",
    "* Initially, We will build simple models to select among the various parametrs such as suitable frame shape. sampling rate and also for tuning hyperparameters such as optimizers etc. In starting models our aim will be to find the right combination of these parameters and  only after selecting these paramets, later we will focus on improving performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZE26rIMnCq7"
   },
   "source": [
    "## Choosing frame shape\n",
    "* We have two types of dimensions present in our datset- 360x360 or 120x160\n",
    "* If we try to make all images 360*360, lower resolution images will get pixalated very much. \n",
    "    * We will prefer downsampling rather than upsampling for image resizing.\n",
    "    * We will thus try two options for image shape : \n",
    "        - img_height*img_width = 64*64 \n",
    "        - img_height*img_width = 120*120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out image with img_height*img_width = 64*64\n",
    "\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "augment = False\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('elu'))\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model1.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('elu'))\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('elu'))\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('elu'))\n",
    "model1.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dropout(0.4))\n",
    "model1.add(Dense(512, activation='elu'))\n",
    "model1.add(Dropout(0.4))\n",
    "model1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model1._name = 'model_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = run(ModelName=model1,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out image with img_height*img_width = 120*120\n",
    "\n",
    "input_shape = (18, 120, 120, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "augment = False\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('elu'))\n",
    "model2.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model2.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('elu'))\n",
    "model2.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('elu'))\n",
    "model2.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('elu'))\n",
    "model2.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(Dense(512, activation='elu'))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model2._name = 'model_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = run(ModelName=model2,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Model_1 is giving slightly better results as compared to model_2. Also, model_1 has less number of total parameters making it comparatively a lighter model.\n",
    "Thus we will use img_height*img_width = 64 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing sample numbers\n",
    "* Each video is a sequence of 30 images.\n",
    "* But we can sample these images to save memory, reduce the computational power and runtime.\n",
    "* In our task,it is logical to say that most of useful information will be present in middle timestamps of a video rather than at the very begining and very end. So we will consider only middle frames of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using images with middle index/timestamp of a sequence (20 frames):\n",
    "\n",
    "input_shape = (20, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "augment = False\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'middle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('elu'))\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model3.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('elu'))\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('elu'))\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('elu'))\n",
    "model3.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(512, activation='elu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model3._name = 'model_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = run(ModelName=model3,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using images with custom index/timestamp of a sequence (roughly 18 frames):\n",
    "\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "augment = False\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('elu'))\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model4.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('elu'))\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('elu'))\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model4.add(Dropout(0.25))\n",
    "\n",
    "model4.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('elu'))\n",
    "model4.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model4.add(Flatten())\n",
    "model4.add(Dropout(0.4))\n",
    "model4.add(Dense(512, activation='elu'))\n",
    "model4.add(Dropout(0.4))\n",
    "model4.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model4._name = 'model_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = run(ModelName=model4,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "custom image indexing is giving comparatively better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing optimizer\n",
    "* We will create new model with SGD optimizer and compare its result with previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 - Using Stochastic gradient descent algorithm:\n",
    "\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "augment = False\n",
    "optimzr = 'sgd'\n",
    "sampling_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('elu'))\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model5.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('elu'))\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('elu'))\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model5.add(Dropout(0.25))\n",
    "\n",
    "model5.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('elu'))\n",
    "model5.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model5.add(Flatten())\n",
    "model5.add(Dropout(0.4))\n",
    "model5.add(Dense(512, activation='elu'))\n",
    "model5.add(Dropout(0.4))\n",
    "model5.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model5._name = 'model_5'\n",
    "\n",
    "history5 = run(ModelName=model5,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Both algorithms are giving similar esults. We will use Adam as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Conv3D Model with higher epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6 - training with higher epocs\n",
    "\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "augment = False\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model \n",
    "model6 = Sequential()\n",
    "model6.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('elu'))\n",
    "model6.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model6.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('elu'))\n",
    "model6.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model6.add(Dropout(0.25))\n",
    "\n",
    "model6.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('elu'))\n",
    "model6.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model6.add(Dropout(0.25))\n",
    "\n",
    "model6.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('elu'))\n",
    "model6.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model6.add(Flatten())\n",
    "model6.add(Dropout(0.4))\n",
    "model6.add(Dense(512, activation='elu'))\n",
    "model6.add(Dropout(0.4))\n",
    "model6.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model6._name = 'model_6'\n",
    "\n",
    "history6 = run(ModelName=model6,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "Model is performing very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing Conv3D Model with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7 - adam with data augmentation\n",
    "\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "augment = True\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=input_shape))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(Activation('elu'))\n",
    "model7.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model7.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(Activation('elu'))\n",
    "model7.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model7.add(Dropout(0.25))\n",
    "\n",
    "model7.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(Activation('elu'))\n",
    "model7.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model7.add(Dropout(0.25))\n",
    "\n",
    "model7.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(Activation('elu'))\n",
    "model7.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model7.add(Flatten())\n",
    "model7.add(Dropout(0.4))\n",
    "model7.add(Dense(512, activation='elu'))\n",
    "model7.add(Dropout(0.4))\n",
    "model7.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model7._name = 'model_7'\n",
    "\n",
    "history7 = run(ModelName=model7,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Model7 is performing good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CNN + GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 8 - CNN + GRU\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "augment = True\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CnnGRU = Sequential()\n",
    "model_CnnGRU.add(TimeDistributed(Conv2D(filters=16,kernel_size=(2,2),padding='same',activation=\"relu\"),input_shape = input_shape))\n",
    "model_CnnGRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_CnnGRU.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_CnnGRU.add(TimeDistributed(Dropout(0.5)))\n",
    "model_CnnGRU.add(TimeDistributed(Conv2D(filters=32,kernel_size=(2,2),padding='same',activation=\"relu\")))\n",
    "model_CnnGRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_CnnGRU.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_CnnGRU.add(TimeDistributed(Dropout(0.5)))\n",
    "model_CnnGRU.add(TimeDistributed(Conv2D(filters=64,kernel_size=(2,2),padding='same',activation=\"relu\")))\n",
    "model_CnnGRU.add(TimeDistributed(BatchNormalization()))\n",
    "model_CnnGRU.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_CnnGRU.add(TimeDistributed(Dropout(0.5)))\n",
    "model_CnnGRU.add(TimeDistributed(Flatten()))\n",
    "model_CnnGRU.add(GRU(64))\n",
    "model_CnnGRU.add(Dropout(0.5))\n",
    "model_CnnGRU.add(Dense(256,activation='relu'))\n",
    "model_CnnGRU.add(Dropout(0.5))\n",
    "model_CnnGRU.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model_CnnGRU._name = 'model_CNN_GRU'\n",
    "\n",
    "history_CnnGRU = run(ModelName=model_CnnGRU,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history_CnnGRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Performance is not satisfactory. \n",
    "We will take help of use transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50 + GRU Model\n",
    "* We will take help of transfer learning to improve the performance.\n",
    "* There are many pretrained models such as inception net, vgg net,GoogleNet etc which offers high accuracy.\n",
    "* But there is a trade-off between model accuracy and efficiency, i.e. the inference time and memory requirement. \n",
    "* But to keep model light and efficient, we will use resnet-50 as it has smaller footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 9 - RESNET50 + GRU\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'\n",
    "augment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet = ResNet50(include_top=False,weights='imagenet',input_shape=input_shape[1:])\n",
    "cnn =Sequential([resnet])\n",
    "cnn.add(Conv2D(16,(2,2),strides=(1,1),padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.6))\n",
    "cnn.add(Conv2D(16,(3,3),strides=(1,1),padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.6))\n",
    "cnn.add(Flatten())\n",
    "\n",
    "model_Resnet50GRU = Sequential()\n",
    "model_Resnet50GRU.add(TimeDistributed(cnn,input_shape=input_shape))\n",
    "model_Resnet50GRU.add(GRU(32,input_shape=(None,input_shape[0],256),return_sequences=True))\n",
    "model_Resnet50GRU.add(GRU(16))\n",
    "model_Resnet50GRU.add(Dropout(0.5))\n",
    "model_Resnet50GRU.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model_Resnet50GRU._name = 'model_Resnet50_GRU'\n",
    "\n",
    "history_Resnet50GRU = run(ModelName=model_Resnet50GRU,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history_Resnet50GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Best weights are saved using checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50 + GRU Model with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 10 - RESNET50 + GRU with data augmentation\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'\n",
    "augment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet50(include_top=False,weights='imagenet',input_shape=input_shape[1:])\n",
    "cnn =Sequential([resnet])\n",
    "cnn.add(Conv2D(16,(2,2),strides=(1,1),padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.6))\n",
    "cnn.add(Conv2D(16,(3,3),strides=(1,1),padding='same'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.6))\n",
    "cnn.add(Flatten())\n",
    "\n",
    "model_ResnetGRU_aug = Sequential()\n",
    "model_ResnetGRU_aug.add(TimeDistributed(cnn,input_shape=input_shape))\n",
    "model_ResnetGRU_aug.add(GRU(32,input_shape=(None,input_shape[0],256),return_sequences=True))\n",
    "model_ResnetGRU_aug.add(GRU(16))\n",
    "model_ResnetGRU_aug.add(Dropout(0.5))\n",
    "model_ResnetGRU_aug.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model_ResnetGRU_aug._name = 'model_Resnet_GRU_aug'\n",
    "\n",
    "history_ResnetGRU_aug = run(ModelName=model_ResnetGRU_aug,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history_ResnetGRU_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Best weights are saved using checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 + GRU with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 11 - VGG16 + GRU with data augmentation\n",
    "input_shape = (18, 64, 64, 3)\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "optimzr = 'adam'\n",
    "sampling_type = 'custom'\n",
    "augment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False,weights='imagenet',input_shape=input_shape[1:])\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "        \n",
    "base_model_ouput = base_model.output\n",
    "x = Flatten()(base_model_ouput)\n",
    "features_1 = Dense(128, activation='relu')(x)\n",
    "# features_2 = Dropout(0.4)(features_1)\n",
    "features_3 = Dense(64, activation='relu')(features_1)\n",
    "features_4 = Dropout(0.4)(features_3)\n",
    "init_model = Model(inputs=base_model.input, outputs=features_4)\n",
    "\n",
    "model_Vgg16GRU = Sequential()\n",
    "model_Vgg16GRU.add(TimeDistributed(init_model, input_shape=input_shape))\n",
    "model_Vgg16GRU.add(GRU(32,return_sequences=True))\n",
    "model_Vgg16GRU.add(GRU(16))\n",
    "model_Vgg16GRU.add(Dropout(0.1))\n",
    "model_Vgg16GRU.add(Dense(32, activation='relu'))\n",
    "model_Vgg16GRU.add(Dropout(0.5))\n",
    "model_Vgg16GRU.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model_Vgg16GRU._name = 'model_Vgg16_GRU'\n",
    "\n",
    "history_Vgg16GRU = run(ModelName=model_Vgg16GRU,batch_size=batch_size,num_epochs=num_epochs,\n",
    "                optimzr=optimzr,sampling_type=sampling_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_performance(history_Vgg16GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Best weights are saved using checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "We can either go with Conv3D model or GRU Models trained using transfer learning since these are giving us better results.\n",
    "We can use any of the saved weights which are not overfitting and are giving better accuracy on train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Neural_Nets_Project_Starter_Code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "23a022f9c9662c4d096dca15f92fe06159fd6cba3a387af0656957db40c935bd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
